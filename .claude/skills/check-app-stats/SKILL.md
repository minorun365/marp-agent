---
name: check-app-stats
description: ã“ã®ã‚¢ãƒ—ãƒªã®åˆ©ç”¨çµ±è¨ˆã‚’ç¢ºèªï¼ˆCognitoãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°ã€AgentCoreå‘¼ã³å‡ºã—å›æ•°ã€Bedrockã‚³ã‚¹ãƒˆã€Tavily APIæ®‹é‡ï¼‰
allowed-tools: Bash(aws:*)
---

# ç’°å¢ƒåˆ©ç”¨çŠ¶æ³ãƒã‚§ãƒƒã‚¯

å„Amplifyç’°å¢ƒï¼ˆmain/kagï¼‰ã®Cognitoãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°ã¨Bedrock AgentCoreãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°ã‚’èª¿æŸ»ã™ã‚‹ã€‚Bedrockã‚³ã‚¹ãƒˆã«ã¤ã„ã¦ã¯devç’°å¢ƒã‚‚å«ã‚ã¦é›†è¨ˆã™ã‚‹ã€‚

mainã¨kagã¯åˆ¥AWSã‚¢ã‚«ã‚¦ãƒ³ãƒˆã§é‹ç”¨ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€ãã‚Œãã‚Œã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã§å€‹åˆ¥ã«ãƒ‡ãƒ¼ã‚¿å–å¾—ã™ã‚‹ã€‚

## å®Ÿè¡Œæ–¹æ³•

**é‡è¦**: ä»¥ä¸‹ã®Bashã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’**ãã®ã¾ã¾1å›ã§å®Ÿè¡Œ**ã™ã‚‹ã“ã¨ã€‚ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚’ä¸¦åˆ—åŒ–ã—ã€1å›ã®æ‰¿èªã§å®Œäº†ã™ã‚‹ã€‚

```bash
#!/bin/bash
set -e

REGION="us-east-1"
PROFILE_MAIN="sandbox"
PROFILE_KAG="kag-sandbox"
OUTPUT_DIR="/tmp/marp-stats"
mkdir -p "$OUTPUT_DIR"

echo "ğŸ“Š Marp Agent åˆ©ç”¨çŠ¶æ³ã‚’å–å¾—ä¸­..."

# SSOã‚»ãƒƒã‚·ãƒ§ãƒ³ç¢ºèªï¼ˆåˆ‡ã‚Œã¦ã„ãŸã‚‰è‡ªå‹•ãƒ­ã‚°ã‚¤ãƒ³ï¼‰
if ! aws sts get-caller-identity --profile $PROFILE_MAIN > /dev/null 2>&1; then
  echo "ğŸ”‘ sandbox ã®SSOã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒç„¡åŠ¹ã§ã™ã€‚ãƒ­ã‚°ã‚¤ãƒ³ã—ã¾ã™..."
  aws sso login --profile $PROFILE_MAIN
fi

KAG_AVAILABLE=true
if ! aws sts get-caller-identity --profile $PROFILE_KAG > /dev/null 2>&1; then
  echo "ğŸ”‘ kag-sandbox ã®SSOã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒç„¡åŠ¹ã§ã™ã€‚ãƒ­ã‚°ã‚¤ãƒ³ã—ã¾ã™..."
  aws sso login --profile $PROFILE_KAG || true
  # ãƒ­ã‚°ã‚¤ãƒ³å¾Œã«å†ç¢ºèª
  aws sts get-caller-identity --profile $PROFILE_KAG > /dev/null 2>&1 || KAG_AVAILABLE=false
  if [ "$KAG_AVAILABLE" = false ]; then
    echo "âš ï¸  kag-sandbox ã®ãƒ­ã‚°ã‚¤ãƒ³ã«å¤±æ•—ã—ã¾ã—ãŸã€‚kagã®ãƒ‡ãƒ¼ã‚¿ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚"
  fi
fi

# ========================================
# 1. ãƒªã‚½ãƒ¼ã‚¹IDå–å¾—
# ========================================
echo "ğŸ” ãƒªã‚½ãƒ¼ã‚¹IDã‚’å–å¾—ä¸­..."

# Cognito User Pool IDå–å¾—
POOL_MAIN=$(aws cognito-idp list-user-pools --max-results 60 --region $REGION --profile $PROFILE_MAIN \
  --query "UserPools[?contains(Name, 'marp-main')].Id" --output text)

POOL_KAG=""
if [ "$KAG_AVAILABLE" = true ]; then
  # kag-sandbox ã§ã¯ãƒ—ãƒ¼ãƒ«åãŒæ±ç”¨çš„ãªãŸã‚ã€CloudFormationå‡ºåŠ›ã‹ã‚‰ç‰¹å®š
  POOL_KAG=$(aws cloudformation describe-stacks \
    --stack-name $(aws cloudformation list-stacks --stack-status-filter CREATE_COMPLETE UPDATE_COMPLETE \
      --region $REGION --profile $PROFILE_KAG \
      --query "StackSummaries[?contains(StackName, 'dt1uykzxnkuoh') && contains(StackName, 'auth')].StackName" --output text) \
    --region $REGION --profile $PROFILE_KAG \
    --query "Stacks[0].Outputs[?contains(OutputKey, 'UserPool') && !contains(OutputKey, 'AppClient')].OutputValue" --output text 2>/dev/null || echo "")
fi

# AgentCore ãƒ­ã‚°ã‚°ãƒ«ãƒ¼ãƒ—åå–å¾—ï¼ˆmain/dev ã¯ sandboxã€kag ã¯ kag-sandboxï¼‰
LOG_MAIN=$(aws logs describe-log-groups \
  --log-group-name-prefix /aws/bedrock-agentcore/runtimes/marp_agent_main \
  --region $REGION --profile $PROFILE_MAIN --query "logGroups[0].logGroupName" --output text)

LOG_KAG="None"
if [ "$KAG_AVAILABLE" = true ]; then
  LOG_KAG=$(aws logs describe-log-groups \
    --log-group-name-prefix /aws/bedrock-agentcore/runtimes/marp_agent \
    --region $REGION --profile $PROFILE_KAG --query "logGroups[0].logGroupName" --output text 2>/dev/null || echo "None")
fi

LOG_DEV=$(aws logs describe-log-groups \
  --log-group-name-prefix /aws/bedrock-agentcore/runtimes/marp_agent_dev \
  --region $REGION --profile $PROFILE_MAIN --query "logGroups[0].logGroupName" --output text 2>/dev/null || echo "None")

# ========================================
# 2. Cognitoãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°å–å¾—ï¼ˆå‰å›å€¤ã¨ã®æ¯”è¼ƒç”¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãï¼‰
# ========================================
echo "ğŸ‘¥ Cognitoãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°ã‚’å–å¾—ä¸­..."
USERS_MAIN=$(aws cognito-idp describe-user-pool --user-pool-id "$POOL_MAIN" --region $REGION --profile $PROFILE_MAIN \
  --query "UserPool.EstimatedNumberOfUsers" --output text 2>/dev/null || echo "0")

USERS_KAG=0
if [ "$KAG_AVAILABLE" = true ] && [ -n "$POOL_KAG" ]; then
  USERS_KAG=$(aws cognito-idp describe-user-pool --user-pool-id "$POOL_KAG" --region $REGION --profile $PROFILE_KAG \
    --query "UserPool.EstimatedNumberOfUsers" --output text 2>/dev/null || echo "0")
fi

# å‰å›å€¤ã‚’èª­ã¿è¾¼ã¿ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°ï¼‰
CACHE_FILE="$OUTPUT_DIR/cognito_cache.json"
PREV_MAIN=0
PREV_KAG=0
PREV_DATE=""
if [ -f "$CACHE_FILE" ]; then
  PREV_MAIN=$(jq -r '.main // 0' "$CACHE_FILE")
  PREV_KAG=$(jq -r '.kag // 0' "$CACHE_FILE")
  PREV_DATE=$(jq -r '.date // ""' "$CACHE_FILE")
fi

# å¢—åŠ æ•°ã‚’è¨ˆç®—
DIFF_MAIN=$((USERS_MAIN - PREV_MAIN))
DIFF_KAG=$((USERS_KAG - PREV_KAG))

# ç¾åœ¨ã®å€¤ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
TODAY=$(TZ=Asia/Tokyo date +%Y-%m-%d)
echo "{\"main\": $USERS_MAIN, \"kag\": $USERS_KAG, \"date\": \"$TODAY\"}" > "$CACHE_FILE"

# kag Cognitoã®æœ€è¿‘è¿½åŠ ãƒ¦ãƒ¼ã‚¶ãƒ¼å–å¾—
if [ "$KAG_AVAILABLE" = true ] && [ -n "$POOL_KAG" ]; then
  echo "ğŸ‘¤ kag Cognitoãƒ¦ãƒ¼ã‚¶ãƒ¼ä¸€è¦§ã‚’å–å¾—ä¸­..."
  aws cognito-idp list-users \
    --user-pool-id "$POOL_KAG" \
    --region $REGION --profile $PROFILE_KAG \
    --output json > "$OUTPUT_DIR/kag_users.json" 2>/dev/null || echo '{"Users":[]}' > "$OUTPUT_DIR/kag_users.json"
else
  echo '{"Users":[]}' > "$OUTPUT_DIR/kag_users.json"
fi

# ========================================
# 3. CloudWatch Logsã‚¯ã‚¨ãƒªã‚’ä¸¦åˆ—é–‹å§‹
# ========================================
echo "ğŸ“ˆ CloudWatch Logsã‚¯ã‚¨ãƒªã‚’ä¸¦åˆ—é–‹å§‹..."
START_7D=$(date -v-7d +%s)
START_24H=$(date -v-24H +%s)
START_28D=$(date -v-28d +%s)  # é€±æ¬¡ãƒˆãƒ¬ãƒ³ãƒ‰ç”¨ï¼ˆ4é€±é–“ï¼‰
END_NOW=$(date +%s)

# OTELãƒ­ã‚°ã‹ã‚‰session.idã‚’parseã—ã¦ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚«ã‚¦ãƒ³ãƒˆï¼ˆUTCã§é›†è¨ˆï¼‰
OTEL_QUERY='parse @message /"session\.id":\s*"(?<sid>[^"]+)"/ | filter ispresent(sid)'

# æ—¥æ¬¡ã‚¯ã‚¨ãƒªé–‹å§‹ï¼ˆmain: sandbox, kag: kag-sandboxï¼‰
Q_DAILY_MAIN=$(aws logs start-query \
  --log-group-name "$LOG_MAIN" \
  --start-time $START_7D --end-time $END_NOW \
  --query-string "$OTEL_QUERY | stats count_distinct(sid) as sessions by datefloor(@timestamp, 1d) as day_utc | sort day_utc asc" \
  --region $REGION --profile $PROFILE_MAIN --query 'queryId' --output text)

Q_DAILY_KAG=""
if [ "$KAG_AVAILABLE" = true ] && [ "$LOG_KAG" != "None" ]; then
  Q_DAILY_KAG=$(aws logs start-query \
    --log-group-name "$LOG_KAG" \
    --start-time $START_7D --end-time $END_NOW \
    --query-string "$OTEL_QUERY | stats count_distinct(sid) as sessions by datefloor(@timestamp, 1d) as day_utc | sort day_utc asc" \
    --region $REGION --profile $PROFILE_KAG --query 'queryId' --output text)
fi

Q_DAILY_DEV=""
if [ "$LOG_DEV" != "None" ]; then
  Q_DAILY_DEV=$(aws logs start-query \
    --log-group-name "$LOG_DEV" \
    --start-time $START_7D --end-time $END_NOW \
    --query-string "$OTEL_QUERY | stats count_distinct(sid) as sessions by datefloor(@timestamp, 1d) as day_utc | sort day_utc asc" \
    --region $REGION --profile $PROFILE_MAIN --query 'queryId' --output text)
fi

# æ™‚é–“åˆ¥ã‚¯ã‚¨ãƒªé–‹å§‹ï¼ˆmain/kag/devä¸¦åˆ—ï¼‰
Q_HOURLY_MAIN=$(aws logs start-query \
  --log-group-name "$LOG_MAIN" \
  --start-time $START_24H --end-time $END_NOW \
  --query-string "$OTEL_QUERY | stats count_distinct(sid) as sessions by datefloor(@timestamp, 1h) as hour_utc | sort hour_utc asc" \
  --region $REGION --profile $PROFILE_MAIN --query 'queryId' --output text)

Q_HOURLY_KAG=""
if [ "$KAG_AVAILABLE" = true ] && [ "$LOG_KAG" != "None" ]; then
  Q_HOURLY_KAG=$(aws logs start-query \
    --log-group-name "$LOG_KAG" \
    --start-time $START_24H --end-time $END_NOW \
    --query-string "$OTEL_QUERY | stats count_distinct(sid) as sessions by datefloor(@timestamp, 1h) as hour_utc | sort hour_utc asc" \
    --region $REGION --profile $PROFILE_KAG --query 'queryId' --output text)
fi

Q_HOURLY_DEV=""
if [ "$LOG_DEV" != "None" ]; then
  Q_HOURLY_DEV=$(aws logs start-query \
    --log-group-name "$LOG_DEV" \
    --start-time $START_24H --end-time $END_NOW \
    --query-string "$OTEL_QUERY | stats count_distinct(sid) as sessions by datefloor(@timestamp, 1h) as hour_utc | sort hour_utc asc" \
    --region $REGION --profile $PROFILE_MAIN --query 'queryId' --output text)
fi

# é€±æ¬¡ã‚¯ã‚¨ãƒªé–‹å§‹ï¼ˆéå»4é€±é–“ï¼‰
Q_WEEKLY_MAIN=$(aws logs start-query \
  --log-group-name "$LOG_MAIN" \
  --start-time $START_28D --end-time $END_NOW \
  --query-string "$OTEL_QUERY | stats count_distinct(sid) as sessions by datefloor(@timestamp, 1d) as day_utc | sort day_utc asc" \
  --region $REGION --profile $PROFILE_MAIN --query 'queryId' --output text)

Q_WEEKLY_KAG=""
if [ "$KAG_AVAILABLE" = true ] && [ "$LOG_KAG" != "None" ]; then
  Q_WEEKLY_KAG=$(aws logs start-query \
    --log-group-name "$LOG_KAG" \
    --start-time $START_28D --end-time $END_NOW \
    --query-string "$OTEL_QUERY | stats count_distinct(sid) as sessions by datefloor(@timestamp, 1d) as day_utc | sort day_utc asc" \
    --region $REGION --profile $PROFILE_KAG --query 'queryId' --output text)
fi

# ========================================
# 4. Bedrockã‚³ã‚¹ãƒˆå–å¾—ï¼ˆã‚¯ã‚¨ãƒªå¾…æ©Ÿä¸­ã«ä¸¦åˆ—å®Ÿè¡Œï¼‰
# ========================================
echo "ğŸ’° Bedrockã‚³ã‚¹ãƒˆã‚’å–å¾—ä¸­..."

# sandbox ã‚¢ã‚«ã‚¦ãƒ³ãƒˆï¼ˆmain+devï¼‰ã®ã‚³ã‚¹ãƒˆï¼ˆã‚¯ãƒ¬ã‚¸ãƒƒãƒˆé©ç”¨å‰ï¼‰
aws ce get-cost-and-usage \
  --time-period Start=$(date -v-7d +%Y-%m-%d),End=$(date +%Y-%m-%d) \
  --granularity DAILY \
  --metrics "UnblendedCost" \
  --filter '{"Dimensions": {"Key": "RECORD_TYPE", "Values": ["Usage"]}}' \
  --group-by Type=DIMENSION,Key=SERVICE \
  --region $REGION --profile $PROFILE_MAIN \
  --output json > "$OUTPUT_DIR/cost.json"

# kag-sandbox ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ã‚³ã‚¹ãƒˆï¼ˆã‚¯ãƒ¬ã‚¸ãƒƒãƒˆé©ç”¨å‰ï¼‰
if [ "$KAG_AVAILABLE" = true ]; then
  aws ce get-cost-and-usage \
    --time-period Start=$(date -v-7d +%Y-%m-%d),End=$(date +%Y-%m-%d) \
    --granularity DAILY \
    --metrics "UnblendedCost" \
    --filter '{"Dimensions": {"Key": "RECORD_TYPE", "Values": ["Usage"]}}' \
    --group-by Type=DIMENSION,Key=SERVICE \
    --region $REGION --profile $PROFILE_KAG \
    --output json > "$OUTPUT_DIR/cost_kag.json"
else
  echo '{"ResultsByTime":[]}' > "$OUTPUT_DIR/cost_kag.json"
fi

# Claude Sonnet 4.5ã®ä½¿ç”¨ã‚¿ã‚¤ãƒ—åˆ¥ã‚³ã‚¹ãƒˆï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹æœåˆ†æç”¨ï¼‰- sandbox
aws ce get-cost-and-usage \
  --time-period Start=$(date -v-7d +%Y-%m-%d),End=$(date +%Y-%m-%d) \
  --granularity DAILY \
  --metrics "UnblendedCost" \
  --filter '{
    "And": [
      {"Dimensions": {"Key": "RECORD_TYPE", "Values": ["Usage"]}},
      {"Dimensions": {"Key": "SERVICE", "Values": ["Claude Sonnet 4.5 (Amazon Bedrock Edition)"]}}
    ]
  }' \
  --group-by Type=DIMENSION,Key=USAGE_TYPE \
  --region $REGION --profile $PROFILE_MAIN \
  --output json > "$OUTPUT_DIR/sonnet_usage.json"

# Claude Sonnet 4.5 - kag-sandbox
if [ "$KAG_AVAILABLE" = true ]; then
  aws ce get-cost-and-usage \
    --time-period Start=$(date -v-7d +%Y-%m-%d),End=$(date +%Y-%m-%d) \
    --granularity DAILY \
    --metrics "UnblendedCost" \
    --filter '{
      "And": [
        {"Dimensions": {"Key": "RECORD_TYPE", "Values": ["Usage"]}},
        {"Dimensions": {"Key": "SERVICE", "Values": ["Claude Sonnet 4.5 (Amazon Bedrock Edition)"]}}
      ]
    }' \
    --group-by Type=DIMENSION,Key=USAGE_TYPE \
    --region $REGION --profile $PROFILE_KAG \
    --output json > "$OUTPUT_DIR/sonnet_usage_kag.json"
else
  echo '{"ResultsByTime":[]}' > "$OUTPUT_DIR/sonnet_usage_kag.json"
fi

# Claude Opus 4.6ã®ä½¿ç”¨ã‚¿ã‚¤ãƒ—åˆ¥ã‚³ã‚¹ãƒˆ - sandbox
aws ce get-cost-and-usage \
  --time-period Start=$(date -v-7d +%Y-%m-%d),End=$(date +%Y-%m-%d) \
  --granularity DAILY \
  --metrics "UnblendedCost" \
  --filter '{
    "And": [
      {"Dimensions": {"Key": "RECORD_TYPE", "Values": ["Usage"]}},
      {"Dimensions": {"Key": "SERVICE", "Values": ["Claude Opus 4.6 (Amazon Bedrock Edition)"]}}
    ]
  }' \
  --group-by Type=DIMENSION,Key=USAGE_TYPE \
  --region $REGION --profile $PROFILE_MAIN \
  --output json > "$OUTPUT_DIR/opus_usage.json"

# Claude Opus 4.6 - kag-sandbox
if [ "$KAG_AVAILABLE" = true ]; then
  aws ce get-cost-and-usage \
    --time-period Start=$(date -v-7d +%Y-%m-%d),End=$(date +%Y-%m-%d) \
    --granularity DAILY \
    --metrics "UnblendedCost" \
    --filter '{
      "And": [
        {"Dimensions": {"Key": "RECORD_TYPE", "Values": ["Usage"]}},
        {"Dimensions": {"Key": "SERVICE", "Values": ["Claude Opus 4.6 (Amazon Bedrock Edition)"]}}
      ]
    }' \
    --group-by Type=DIMENSION,Key=USAGE_TYPE \
    --region $REGION --profile $PROFILE_KAG \
    --output json > "$OUTPUT_DIR/opus_usage_kag.json"
else
  echo '{"ResultsByTime":[]}' > "$OUTPUT_DIR/opus_usage_kag.json"
fi

# é€±æ¬¡ã‚³ã‚¹ãƒˆå–å¾—ï¼ˆéå»4é€±é–“ï¼‰- sandbox
aws ce get-cost-and-usage \
  --time-period Start=$(date -v-28d +%Y-%m-%d),End=$(date +%Y-%m-%d) \
  --granularity DAILY \
  --metrics "UnblendedCost" \
  --filter '{"Dimensions": {"Key": "RECORD_TYPE", "Values": ["Usage"]}}' \
  --group-by Type=DIMENSION,Key=SERVICE \
  --region $REGION --profile $PROFILE_MAIN \
  --output json > "$OUTPUT_DIR/weekly_cost.json"

# é€±æ¬¡ã‚³ã‚¹ãƒˆ - kag-sandbox
if [ "$KAG_AVAILABLE" = true ]; then
  aws ce get-cost-and-usage \
    --time-period Start=$(date -v-28d +%Y-%m-%d),End=$(date +%Y-%m-%d) \
    --granularity DAILY \
    --metrics "UnblendedCost" \
    --filter '{"Dimensions": {"Key": "RECORD_TYPE", "Values": ["Usage"]}}' \
    --group-by Type=DIMENSION,Key=SERVICE \
    --region $REGION --profile $PROFILE_KAG \
    --output json > "$OUTPUT_DIR/weekly_cost_kag.json"
else
  echo '{"ResultsByTime":[]}' > "$OUTPUT_DIR/weekly_cost_kag.json"
fi

# ========================================
# 4.5 Tavily APIåˆ©ç”¨é‡å–å¾—
# ========================================
echo "ğŸ” Tavily APIåˆ©ç”¨é‡ã‚’å–å¾—ä¸­..."

ENV_FILE="$PWD/.env"
TAVILY_KEYS=""
if [ -f "$ENV_FILE" ]; then
  TAVILY_KEYS=$(grep '^TAVILY_API_KEYS=' "$ENV_FILE" | cut -d'=' -f2)
fi

if [ -n "$TAVILY_KEYS" ]; then
  echo "$TAVILY_KEYS" | tr ',' '\n' > "$OUTPUT_DIR/tavily_keys.tmp"
  TAVILY_KEY_COUNT=0
  while IFS= read -r KEY; do
    [ -z "$KEY" ] && continue
    TAVILY_KEY_COUNT=$((TAVILY_KEY_COUNT + 1))
    curl -s --max-time 5 "https://api.tavily.com/usage" -H "Authorization: Bearer $KEY" \
      > "$OUTPUT_DIR/tavily_key${TAVILY_KEY_COUNT}.json" 2>/dev/null || echo '{}' > "$OUTPUT_DIR/tavily_key${TAVILY_KEY_COUNT}.json"
  done < "$OUTPUT_DIR/tavily_keys.tmp"
  rm -f "$OUTPUT_DIR/tavily_keys.tmp"
else
  TAVILY_KEY_COUNT=0
fi

# ========================================
# 5. ã‚¯ã‚¨ãƒªçµæœå–å¾—ï¼ˆ10ç§’å¾…æ©Ÿå¾Œï¼‰
# ========================================
echo "â³ ã‚¯ã‚¨ãƒªå®Œäº†ã‚’å¾…æ©Ÿä¸­..."
sleep 10

echo "ğŸ“¥ ã‚¯ã‚¨ãƒªçµæœã‚’å–å¾—ä¸­..."
aws logs get-query-results --query-id "$Q_DAILY_MAIN" --region $REGION --profile $PROFILE_MAIN > "$OUTPUT_DIR/daily_main.json"
aws logs get-query-results --query-id "$Q_HOURLY_MAIN" --region $REGION --profile $PROFILE_MAIN > "$OUTPUT_DIR/hourly_main.json"

if [ -n "$Q_DAILY_KAG" ]; then
  aws logs get-query-results --query-id "$Q_DAILY_KAG" --region $REGION --profile $PROFILE_KAG > "$OUTPUT_DIR/daily_kag.json"
  aws logs get-query-results --query-id "$Q_HOURLY_KAG" --region $REGION --profile $PROFILE_KAG > "$OUTPUT_DIR/hourly_kag.json"
else
  echo '{"results":[]}' > "$OUTPUT_DIR/daily_kag.json"
  echo '{"results":[]}' > "$OUTPUT_DIR/hourly_kag.json"
fi

if [ -n "$Q_DAILY_DEV" ]; then
  aws logs get-query-results --query-id "$Q_DAILY_DEV" --region $REGION --profile $PROFILE_MAIN > "$OUTPUT_DIR/daily_dev.json"
  aws logs get-query-results --query-id "$Q_HOURLY_DEV" --region $REGION --profile $PROFILE_MAIN > "$OUTPUT_DIR/hourly_dev.json"
else
  echo '{"results":[]}' > "$OUTPUT_DIR/daily_dev.json"
  echo '{"results":[]}' > "$OUTPUT_DIR/hourly_dev.json"
fi

aws logs get-query-results --query-id "$Q_WEEKLY_MAIN" --region $REGION --profile $PROFILE_MAIN > "$OUTPUT_DIR/weekly_main.json"
if [ -n "$Q_WEEKLY_KAG" ]; then
  aws logs get-query-results --query-id "$Q_WEEKLY_KAG" --region $REGION --profile $PROFILE_KAG > "$OUTPUT_DIR/weekly_kag.json"
else
  echo '{"results":[]}' > "$OUTPUT_DIR/weekly_kag.json"
fi

# ========================================
# 6. çµæœå‡ºåŠ›
# ========================================
echo ""
echo "=========================================="
echo "ğŸ“Š MARP AGENT åˆ©ç”¨çŠ¶æ³ãƒ¬ãƒãƒ¼ãƒˆ"
echo "=========================================="
echo ""

# ========================================
# ç›´è¿‘12æ™‚é–“ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°ã‚’è¡¨å½¢å¼ã§è¡¨ç¤º
# ========================================
CURRENT_JST_HOUR=$(TZ=Asia/Tokyo date +%H)

# UTCã®æ™‚åˆ»ã‚’JSTã«å¤‰æ›ã—ã¦ãƒãƒƒãƒ—ã‚’ä½œæˆï¼ˆç›´è¿‘12æ™‚é–“ç”¨ï¼‰
declare -A MAIN_MAP_12H
declare -A KAG_MAP_12H
declare -A DEV_MAP_12H

# mainã®ãƒ‡ãƒ¼ã‚¿ã‚’JSTå¤‰æ›ã—ã¦ãƒãƒƒãƒ—ã«æ ¼ç´
while IFS= read -r line; do
  if [ -n "$line" ]; then
    UTC_HOUR=$(echo "$line" | cut -d'|' -f1)
    SESSIONS=$(echo "$line" | cut -d'|' -f2)
    JST_HOUR=$(( (10#$UTC_HOUR + 9) % 24 ))
    JST_HOUR_STR=$(printf "%02d" $JST_HOUR)
    MAIN_MAP_12H[$JST_HOUR_STR]=$SESSIONS
  fi
done < <(jq -r '.results[] |
  (.[] | select(.field == "hour_utc") | .value[11:13]) as $hour |
  (.[] | select(.field == "sessions") | .value) as $sessions |
  "\($hour)|\($sessions)"
' "$OUTPUT_DIR/hourly_main.json" 2>/dev/null)

# kagã®ãƒ‡ãƒ¼ã‚¿ã‚’JSTå¤‰æ›ã—ã¦ãƒãƒƒãƒ—ã«æ ¼ç´
while IFS= read -r line; do
  if [ -n "$line" ]; then
    UTC_HOUR=$(echo "$line" | cut -d'|' -f1)
    SESSIONS=$(echo "$line" | cut -d'|' -f2)
    JST_HOUR=$(( (10#$UTC_HOUR + 9) % 24 ))
    JST_HOUR_STR=$(printf "%02d" $JST_HOUR)
    KAG_MAP_12H[$JST_HOUR_STR]=$SESSIONS
  fi
done < <(jq -r '.results[] |
  (.[] | select(.field == "hour_utc") | .value[11:13]) as $hour |
  (.[] | select(.field == "sessions") | .value) as $sessions |
  "\($hour)|\($sessions)"
' "$OUTPUT_DIR/hourly_kag.json" 2>/dev/null)

# devã®ãƒ‡ãƒ¼ã‚¿ã‚’JSTå¤‰æ›ã—ã¦ãƒãƒƒãƒ—ã«æ ¼ç´
while IFS= read -r line; do
  if [ -n "$line" ]; then
    UTC_HOUR=$(echo "$line" | cut -d'|' -f1)
    SESSIONS=$(echo "$line" | cut -d'|' -f2)
    JST_HOUR=$(( (10#$UTC_HOUR + 9) % 24 ))
    JST_HOUR_STR=$(printf "%02d" $JST_HOUR)
    DEV_MAP_12H[$JST_HOUR_STR]=$SESSIONS
  fi
done < <(jq -r '.results[] |
  (.[] | select(.field == "hour_utc") | .value[11:13]) as $hour |
  (.[] | select(.field == "sessions") | .value) as $sessions |
  "\($hour)|\($sessions)"
' "$OUTPUT_DIR/hourly_dev.json" 2>/dev/null)

echo "ğŸ”¥ ç›´è¿‘12æ™‚é–“ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°ï¼ˆJSTï¼‰"
echo ""
echo "  æ™‚åˆ»   | main | kag  | dev  | åˆè¨ˆ"
echo "  -------|------|------|------|------"

SUM_MAIN_12H=0
SUM_KAG_12H=0
SUM_DEV_12H=0

# ç›´è¿‘12æ™‚é–“ã‚’å¤ã„é †ã«è¡¨ç¤º
for i in $(seq 11 -1 0); do
  HOUR=$(( (10#$CURRENT_JST_HOUR - i + 24) % 24 ))
  HOUR_STR=$(printf "%02d" $HOUR)

  MAIN_C=${MAIN_MAP_12H[$HOUR_STR]:-0}
  KAG_C=${KAG_MAP_12H[$HOUR_STR]:-0}
  DEV_C=${DEV_MAP_12H[$HOUR_STR]:-0}
  TOTAL_C=$((MAIN_C + KAG_C + DEV_C))

  SUM_MAIN_12H=$((SUM_MAIN_12H + MAIN_C))
  SUM_KAG_12H=$((SUM_KAG_12H + KAG_C))
  SUM_DEV_12H=$((SUM_DEV_12H + DEV_C))

  printf "  %s:00 | %4d | %4d | %4d | %4d\n" "$HOUR_STR" "$MAIN_C" "$KAG_C" "$DEV_C" "$TOTAL_C"
done

SUM_TOTAL_12H=$((SUM_MAIN_12H + SUM_KAG_12H + SUM_DEV_12H))
echo "  -------|------|------|------|------"
printf "  åˆè¨ˆ   | %4d | %4d | %4d | %4d\n" "$SUM_MAIN_12H" "$SUM_KAG_12H" "$SUM_DEV_12H" "$SUM_TOTAL_12H"
echo ""

echo "ğŸ‘¥ Cognitoãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°"
if [ -n "$PREV_DATE" ] && [ "$PREV_DATE" != "$TODAY" ]; then
  # å‰å›è¨˜éŒ²ãŒåˆ¥æ—¥ã®å ´åˆã€å¢—æ¸›ã‚’è¡¨ç¤º
  DIFF_MAIN_STR=""
  DIFF_KAG_STR=""
  DIFF_TOTAL=$((DIFF_MAIN + DIFF_KAG))
  if [ $DIFF_MAIN -gt 0 ]; then DIFF_MAIN_STR=" (+$DIFF_MAIN)"; elif [ $DIFF_MAIN -lt 0 ]; then DIFF_MAIN_STR=" ($DIFF_MAIN)"; fi
  if [ $DIFF_KAG -gt 0 ]; then DIFF_KAG_STR=" (+$DIFF_KAG)"; elif [ $DIFF_KAG -lt 0 ]; then DIFF_KAG_STR=" ($DIFF_KAG)"; fi
  DIFF_TOTAL_STR=""
  if [ $DIFF_TOTAL -gt 0 ]; then DIFF_TOTAL_STR=" (+$DIFF_TOTAL)"; elif [ $DIFF_TOTAL -lt 0 ]; then DIFF_TOTAL_STR=" ($DIFF_TOTAL)"; fi
  echo "  main: $USERS_MAIN äºº$DIFF_MAIN_STR"
  echo "  kag:  $USERS_KAG äºº$DIFF_KAG_STR"
  echo "  åˆè¨ˆ: $((USERS_MAIN + USERS_KAG)) äºº$DIFF_TOTAL_STR"
  echo "  ï¼ˆå‰å›è¨˜éŒ²: $PREV_DATEï¼‰"
else
  # åˆå›ã¾ãŸã¯åŒæ—¥ã®å ´åˆã¯å¢—æ¸›ãªã—
  echo "  main: $USERS_MAIN äºº"
  echo "  kag:  $USERS_KAG äºº"
  echo "  åˆè¨ˆ: $((USERS_MAIN + USERS_KAG)) äºº"
  if [ -z "$PREV_DATE" ]; then
    echo "  ï¼ˆåˆå›è¨˜éŒ² - æ¬¡å›ä»¥é™å¢—æ¸›ã‚’è¡¨ç¤ºï¼‰"
  fi
fi

# kag æœ€è¿‘è¿½åŠ ã•ã‚ŒãŸãƒ¦ãƒ¼ã‚¶ãƒ¼
KAG_USER_COUNT=$(jq '.Users | length' "$OUTPUT_DIR/kag_users.json" 2>/dev/null || echo 0)
if [ "$KAG_USER_COUNT" -gt 0 ]; then
  echo ""
  echo "  [kag æœ€è¿‘è¿½åŠ ã•ã‚ŒãŸãƒ¦ãƒ¼ã‚¶ãƒ¼]"
  jq -r '
    .Users | sort_by(.UserCreateDate) | reverse | .[] |
    (.UserCreateDate | split("T")[0]) as $date |
    ((.Attributes // [])[] | select(.Name == "email") | .Value) as $email |
    "  \($date): \($email // "emailæœªè¨­å®š")"
  ' "$OUTPUT_DIR/kag_users.json" 2>/dev/null | head -10
fi
echo ""

echo "ğŸ“ˆ æ—¥æ¬¡ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°ï¼ˆéå»7æ—¥é–“ï¼‰"
echo "[main]"
jq -r '.results[] |
  (.[] | select(.field == "day_utc") | .value | split(" ")[0]) as $date |
  (.[] | select(.field == "sessions") | .value) as $sessions |
  "  \($date): \($sessions) å›"
' "$OUTPUT_DIR/daily_main.json"
TOTAL_MAIN=$(jq '[.results[][] | select(.field == "sessions") | .value | tonumber] | add // 0' "$OUTPUT_DIR/daily_main.json")
echo "  åˆè¨ˆ: $TOTAL_MAIN å›"
echo ""
echo "[kag]"
KAG_DAILY_COUNT=$(jq '.results | length' "$OUTPUT_DIR/daily_kag.json")
if [ "$KAG_DAILY_COUNT" -gt 0 ]; then
  jq -r '.results[] |
    (.[] | select(.field == "day_utc") | .value | split(" ")[0]) as $date |
    (.[] | select(.field == "sessions") | .value) as $sessions |
    "  \($date): \($sessions) å›"
  ' "$OUTPUT_DIR/daily_kag.json"
  TOTAL_KAG=$(jq '[.results[][] | select(.field == "sessions") | .value | tonumber] | add // 0' "$OUTPUT_DIR/daily_kag.json")
else
  TOTAL_KAG=0
  echo "  ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³ãªã—ï¼‰"
fi
echo "  åˆè¨ˆ: $TOTAL_KAG å›"
echo ""
echo "[dev]"
DEV_DAILY_COUNT=$(jq '.results | length' "$OUTPUT_DIR/daily_dev.json")
if [ "$DEV_DAILY_COUNT" -gt 0 ]; then
  jq -r '.results[] |
    (.[] | select(.field == "day_utc") | .value | split(" ")[0]) as $date |
    (.[] | select(.field == "sessions") | .value) as $sessions |
    "  \($date): \($sessions) å›"
  ' "$OUTPUT_DIR/daily_dev.json"
  TOTAL_DEV=$(jq '[.results[][] | select(.field == "sessions") | .value | tonumber] | add // 0' "$OUTPUT_DIR/daily_dev.json")
else
  TOTAL_DEV=0
  echo "  ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³ãªã—ï¼‰"
fi
echo "  åˆè¨ˆ: $TOTAL_DEV å›"
echo ""

echo "â° æ™‚é–“åˆ¥ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°ï¼ˆç›´è¿‘24æ™‚é–“ãƒ»JSTï¼‰"
echo ""
echo "        [main]              [kag]               [dev]"
echo "  æ™‚åˆ»  |  ã‚°ãƒ©ãƒ•     | å›æ•° |  ã‚°ãƒ©ãƒ•     | å›æ•° |  ã‚°ãƒ©ãƒ•     | å›æ•°"
echo "  ------|-------------|------|-------------|------|-------------|------"

# UTCã®æ™‚åˆ»ã‚’JSTã«å¤‰æ›ã—ã¦ãƒãƒƒãƒ—ã‚’ä½œæˆ
declare -A MAIN_MAP
declare -A KAG_MAP
declare -A DEV_MAP

# mainã®ãƒ‡ãƒ¼ã‚¿ã‚’JSTå¤‰æ›ã—ã¦ãƒãƒƒãƒ—ã«æ ¼ç´
while IFS= read -r line; do
  if [ -n "$line" ]; then
    UTC_HOUR=$(echo "$line" | cut -d'|' -f1)
    SESSIONS=$(echo "$line" | cut -d'|' -f2)
    JST_HOUR=$(( (10#$UTC_HOUR + 9) % 24 ))
    JST_HOUR_STR=$(printf "%02d" $JST_HOUR)
    MAIN_MAP[$JST_HOUR_STR]=$SESSIONS
  fi
done < <(jq -r '.results[] |
  (.[] | select(.field == "hour_utc") | .value[11:13]) as $hour |
  (.[] | select(.field == "sessions") | .value) as $sessions |
  "\($hour)|\($sessions)"
' "$OUTPUT_DIR/hourly_main.json" 2>/dev/null)

# kagã®ãƒ‡ãƒ¼ã‚¿ã‚’JSTå¤‰æ›ã—ã¦ãƒãƒƒãƒ—ã«æ ¼ç´
while IFS= read -r line; do
  if [ -n "$line" ]; then
    UTC_HOUR=$(echo "$line" | cut -d'|' -f1)
    SESSIONS=$(echo "$line" | cut -d'|' -f2)
    JST_HOUR=$(( (10#$UTC_HOUR + 9) % 24 ))
    JST_HOUR_STR=$(printf "%02d" $JST_HOUR)
    KAG_MAP[$JST_HOUR_STR]=$SESSIONS
  fi
done < <(jq -r '.results[] |
  (.[] | select(.field == "hour_utc") | .value[11:13]) as $hour |
  (.[] | select(.field == "sessions") | .value) as $sessions |
  "\($hour)|\($sessions)"
' "$OUTPUT_DIR/hourly_kag.json" 2>/dev/null)

# devã®ãƒ‡ãƒ¼ã‚¿ã‚’JSTå¤‰æ›ã—ã¦ãƒãƒƒãƒ—ã«æ ¼ç´
while IFS= read -r line; do
  if [ -n "$line" ]; then
    UTC_HOUR=$(echo "$line" | cut -d'|' -f1)
    SESSIONS=$(echo "$line" | cut -d'|' -f2)
    JST_HOUR=$(( (10#$UTC_HOUR + 9) % 24 ))
    JST_HOUR_STR=$(printf "%02d" $JST_HOUR)
    DEV_MAP[$JST_HOUR_STR]=$SESSIONS
  fi
done < <(jq -r '.results[] |
  (.[] | select(.field == "hour_utc") | .value[11:13]) as $hour |
  (.[] | select(.field == "sessions") | .value) as $sessions |
  "\($hour)|\($sessions)"
' "$OUTPUT_DIR/hourly_dev.json" 2>/dev/null)

# ç¾åœ¨æ™‚åˆ»ï¼ˆJSTï¼‰ã‹ã‚‰24æ™‚é–“åˆ†ã‚’å¤ã„é †ã«è¡¨ç¤º
CURRENT_HOUR=$(TZ=Asia/Tokyo date +%H)
for i in $(seq 23 -1 0); do
  HOUR=$(( (10#$CURRENT_HOUR - i + 24) % 24 ))
  HOUR_STR=$(printf "%02d" $HOUR)

  # mainã®ã‚«ã‚¦ãƒ³ãƒˆå–å¾—
  MAIN_COUNT=${MAIN_MAP[$HOUR_STR]:-0}
  MAIN_BARS=$(( MAIN_COUNT / 2 ))
  [ $MAIN_BARS -gt 10 ] && MAIN_BARS=10
  if [ $MAIN_BARS -gt 0 ]; then
    MAIN_BAR=$(printf 'â–ˆ%.0s' $(seq 1 $MAIN_BARS))
  else
    MAIN_BAR=""
  fi

  # kagã®ã‚«ã‚¦ãƒ³ãƒˆå–å¾—
  KAG_COUNT=${KAG_MAP[$HOUR_STR]:-0}
  KAG_BARS=$(( KAG_COUNT / 2 ))
  [ $KAG_BARS -gt 10 ] && KAG_BARS=10
  if [ $KAG_BARS -gt 0 ]; then
    KAG_BAR=$(printf 'â–ˆ%.0s' $(seq 1 $KAG_BARS))
  else
    KAG_BAR=""
  fi

  # devã®ã‚«ã‚¦ãƒ³ãƒˆå–å¾—
  DEV_COUNT=${DEV_MAP[$HOUR_STR]:-0}
  DEV_BARS=$(( DEV_COUNT / 2 ))
  [ $DEV_BARS -gt 10 ] && DEV_BARS=10
  if [ $DEV_BARS -gt 0 ]; then
    DEV_BAR=$(printf 'â–ˆ%.0s' $(seq 1 $DEV_BARS))
  else
    DEV_BAR=""
  fi

  printf "  %s:00 | %-11s | %4d | %-11s | %4d | %-11s | %4d\n" "$HOUR_STR" "$MAIN_BAR" "$MAIN_COUNT" "$KAG_BAR" "$KAG_COUNT" "$DEV_BAR" "$DEV_COUNT"
done
echo ""

echo "ğŸ’° Bedrockã‚³ã‚¹ãƒˆï¼ˆéå»7æ—¥é–“ãƒ»æ—¥åˆ¥ãƒ»ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆé©ç”¨å‰ï¼‰"
echo "[sandbox (main+dev)]"
jq -r '
  .ResultsByTime[] |
  .TimePeriod.Start as $date |
  [.Groups[] | select(.Keys[0] | contains("Claude") or contains("Bedrock")) | .Metrics.UnblendedCost.Amount | tonumber] |
  add // 0 |
  "  \($date): $\(. | . * 100 | floor / 100)"
' "$OUTPUT_DIR/cost.json"

TOTAL_COST_SANDBOX=$(jq -r '
  [.ResultsByTime[].Groups[] | select(.Keys[0] | contains("Claude") or contains("Bedrock")) | .Metrics.UnblendedCost.Amount | tonumber] | add // 0
' "$OUTPUT_DIR/cost.json")
echo "  å°è¨ˆ: \$$TOTAL_COST_SANDBOX"
echo ""

TOTAL_COST_KAG=0
if [ "$KAG_AVAILABLE" = true ]; then
  echo "[kag]"
  jq -r '
    .ResultsByTime[] |
    .TimePeriod.Start as $date |
    [.Groups[] | select(.Keys[0] | contains("Claude") or contains("Bedrock")) | .Metrics.UnblendedCost.Amount | tonumber] |
    add // 0 |
    "  \($date): $\(. | . * 100 | floor / 100)"
  ' "$OUTPUT_DIR/cost_kag.json"
  TOTAL_COST_KAG=$(jq -r '
    [.ResultsByTime[].Groups[] | select(.Keys[0] | contains("Claude") or contains("Bedrock")) | .Metrics.UnblendedCost.Amount | tonumber] | add // 0
  ' "$OUTPUT_DIR/cost_kag.json")
  echo "  å°è¨ˆ: \$$TOTAL_COST_KAG"
  echo ""
fi

TOTAL_COST=$(echo "$TOTAL_COST_SANDBOX + $TOTAL_COST_KAG" | bc)
echo "  é€±é–“åˆè¨ˆ: \$$TOTAL_COST"
echo ""

# ========================================
# ç’°å¢ƒåˆ¥ x ãƒ¢ãƒ‡ãƒ«åˆ¥ã‚³ã‚¹ãƒˆï¼ˆå®Ÿã‚³ã‚¹ãƒˆï¼‰
# ========================================
echo "ğŸ’° Bedrockã‚³ã‚¹ãƒˆå†…è¨³ï¼ˆéå»7æ—¥é–“ãƒ»ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆé©ç”¨å‰ï¼‰"
echo ""

# sandbox ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ãƒ¢ãƒ‡ãƒ«åˆ¥ã‚³ã‚¹ãƒˆ
SONNET_COST_SANDBOX=$(jq -r '
  [.ResultsByTime[].Groups[] | select(.Keys[0] | contains("Claude Sonnet 4.5")) | .Metrics.UnblendedCost.Amount | tonumber] | add // 0
' "$OUTPUT_DIR/cost.json")
OPUS_COST_SANDBOX=$(jq -r '
  [.ResultsByTime[].Groups[] | select(.Keys[0] | contains("Claude Opus")) | .Metrics.UnblendedCost.Amount | tonumber] | add // 0
' "$OUTPUT_DIR/cost.json")
KIMI_COST_SANDBOX=$(jq -r '
  [.ResultsByTime[].Groups[] | select(.Keys[0] | contains("Kimi")) | .Metrics.UnblendedCost.Amount | tonumber] | add // 0
' "$OUTPUT_DIR/cost.json")
OTHER_COST_SANDBOX=$(jq -r '
  [.ResultsByTime[].Groups[] | select((.Keys[0] | contains("Bedrock") or contains("Claude")) and (.Keys[0] | contains("Claude Sonnet 4.5") | not) and (.Keys[0] | contains("Claude Opus") | not) and (.Keys[0] | contains("Kimi") | not)) | .Metrics.UnblendedCost.Amount | tonumber] | add // 0
' "$OUTPUT_DIR/cost.json")

# kag-sandbox ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ãƒ¢ãƒ‡ãƒ«åˆ¥ã‚³ã‚¹ãƒˆ
SONNET_COST_KAG_REAL=$(jq -r '
  [.ResultsByTime[].Groups[] | select(.Keys[0] | contains("Claude Sonnet 4.5")) | .Metrics.UnblendedCost.Amount | tonumber] | add // 0
' "$OUTPUT_DIR/cost_kag.json")
OPUS_COST_KAG_REAL=$(jq -r '
  [.ResultsByTime[].Groups[] | select(.Keys[0] | contains("Claude Opus")) | .Metrics.UnblendedCost.Amount | tonumber] | add // 0
' "$OUTPUT_DIR/cost_kag.json")
KIMI_COST_KAG_REAL=$(jq -r '
  [.ResultsByTime[].Groups[] | select(.Keys[0] | contains("Kimi")) | .Metrics.UnblendedCost.Amount | tonumber] | add // 0
' "$OUTPUT_DIR/cost_kag.json")
OTHER_COST_KAG_REAL=$(jq -r '
  [.ResultsByTime[].Groups[] | select((.Keys[0] | contains("Bedrock") or contains("Claude")) and (.Keys[0] | contains("Claude Sonnet 4.5") | not) and (.Keys[0] | contains("Claude Opus") | not) and (.Keys[0] | contains("Kimi") | not)) | .Metrics.UnblendedCost.Amount | tonumber] | add // 0
' "$OUTPUT_DIR/cost_kag.json")

# sandbox å†…ã® main/dev æ¯”ç‡ï¼ˆdev ãŒã‚ã‚‹å ´åˆã®ã¿åˆ†å‰²ï¼‰
SANDBOX_SESSIONS=$((TOTAL_MAIN + TOTAL_DEV))
if [ "$SANDBOX_SESSIONS" -gt 0 ] && [ "$TOTAL_DEV" -gt 0 ]; then
  MAIN_RATIO=$(echo "scale=6; $TOTAL_MAIN / $SANDBOX_SESSIONS" | bc)
  DEV_RATIO=$(echo "scale=6; $TOTAL_DEV / $SANDBOX_SESSIONS" | bc)

  S_MAIN=$(printf "%.2f" $(echo "$SONNET_COST_SANDBOX * $MAIN_RATIO" | bc -l))
  S_DEV=$(printf "%.2f" $(echo "$SONNET_COST_SANDBOX * $DEV_RATIO" | bc -l))
  O_MAIN=$(printf "%.2f" $(echo "$OPUS_COST_SANDBOX * $MAIN_RATIO" | bc -l))
  O_DEV=$(printf "%.2f" $(echo "$OPUS_COST_SANDBOX * $DEV_RATIO" | bc -l))
  K_MAIN=$(printf "%.2f" $(echo "$KIMI_COST_SANDBOX * $MAIN_RATIO" | bc -l))
  K_DEV=$(printf "%.2f" $(echo "$KIMI_COST_SANDBOX * $DEV_RATIO" | bc -l))
  OT_MAIN=$(printf "%.2f" $(echo "$OTHER_COST_SANDBOX * $MAIN_RATIO" | bc -l))
  OT_DEV=$(printf "%.2f" $(echo "$OTHER_COST_SANDBOX * $DEV_RATIO" | bc -l))
  ENV_MAIN=$(printf "%.2f" $(echo "$TOTAL_COST_SANDBOX * $MAIN_RATIO" | bc -l))
  ENV_DEV=$(printf "%.2f" $(echo "$TOTAL_COST_SANDBOX * $DEV_RATIO" | bc -l))
else
  # dev ãŒãªã„å ´åˆã¯ sandbox = main
  S_MAIN=$(printf "%.2f" $SONNET_COST_SANDBOX)
  S_DEV="0.00"
  O_MAIN=$(printf "%.2f" $OPUS_COST_SANDBOX)
  O_DEV="0.00"
  K_MAIN=$(printf "%.2f" $KIMI_COST_SANDBOX)
  K_DEV="0.00"
  OT_MAIN=$(printf "%.2f" $OTHER_COST_SANDBOX)
  OT_DEV="0.00"
  ENV_MAIN=$(printf "%.2f" $TOTAL_COST_SANDBOX)
  ENV_DEV="0.00"
fi

# kag ã¯å®Ÿã‚³ã‚¹ãƒˆ
S_KAG=$(printf "%.2f" $SONNET_COST_KAG_REAL)
O_KAG=$(printf "%.2f" $OPUS_COST_KAG_REAL)
K_KAG=$(printf "%.2f" $KIMI_COST_KAG_REAL)
OT_KAG=$(printf "%.2f" $OTHER_COST_KAG_REAL)
ENV_KAG=$(printf "%.2f" $TOTAL_COST_KAG)

# åˆè¨ˆ
S_TOTAL=$(printf "%.2f" $(echo "$SONNET_COST_SANDBOX + $SONNET_COST_KAG_REAL" | bc))
O_TOTAL=$(printf "%.2f" $(echo "$OPUS_COST_SANDBOX + $OPUS_COST_KAG_REAL" | bc))
K_TOTAL=$(printf "%.2f" $(echo "$KIMI_COST_SANDBOX + $KIMI_COST_KAG_REAL" | bc))
OT_TOTAL=$(printf "%.2f" $(echo "$OTHER_COST_SANDBOX + $OTHER_COST_KAG_REAL" | bc))
ENV_TOTAL=$(printf "%.2f" $(echo "$TOTAL_COST" | bc -l))

# æœˆé–“æ¨å®š
M_MAIN=$(printf "%.0f" $(echo "$ENV_MAIN * 4" | bc -l))
M_KAG=$(printf "%.0f" $(echo "$ENV_KAG * 4" | bc -l))
M_DEV=$(printf "%.0f" $(echo "$ENV_DEV * 4" | bc -l))
M_TOTAL=$(printf "%.0f" $(echo "$ENV_TOTAL * 4" | bc -l))

echo "  â€» ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆé©ç”¨å‰ã®åˆ©ç”¨ã‚³ã‚¹ãƒˆï¼ˆRECORD_TYPE=Usageã§ãƒ•ã‚£ãƒ«ã‚¿ï¼‰"
echo ""
printf "  %-16s | %8s | %8s | %8s | %8s\n" "ãƒ¢ãƒ‡ãƒ«" "main" "kag" "dev" "åˆè¨ˆ"
printf "  %-16s-|----------|----------|----------|----------\n" "----------------"
printf "  %-16s | %8s | %8s | %8s | %8s\n" "Sonnet 4.5" "\$$S_MAIN" "\$$S_KAG" "\$$S_DEV" "\$$S_TOTAL"
printf "  %-16s | %8s | %8s | %8s | %8s\n" "Opus 4.6" "\$$O_MAIN" "\$$O_KAG" "\$$O_DEV" "\$$O_TOTAL"
printf "  %-16s | %8s | %8s | %8s | %8s\n" "Kimi K2" "\$$K_MAIN" "\$$K_KAG" "\$$K_DEV" "\$$K_TOTAL"
printf "  %-16s | %8s | %8s | %8s | %8s\n" "ãã®ä»–" "\$$OT_MAIN" "\$$OT_KAG" "\$$OT_DEV" "\$$OT_TOTAL"
printf "  %-16s-|----------|----------|----------|----------\n" "----------------"
printf "  %-16s | %8s | %8s | %8s | %8s\n" "é€±é–“åˆè¨ˆ" "\$$ENV_MAIN" "\$$ENV_KAG" "\$$ENV_DEV" "\$$ENV_TOTAL"
printf "  %-16s | %7s | %7s | %7s | %7s\n" "æœˆé–“æ¨å®š" "\$$M_MAIN" "\$$M_KAG" "\$$M_DEV" "\$$M_TOTAL"
echo ""
echo "  â€» Kimi K2ã¯ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆé©ç”¨ã§å®Ÿè³ª\$0"
echo ""

# ========================================
# 1ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚ãŸã‚Šã®ã‚³ã‚¹ãƒˆåˆ†æ
# ========================================
echo "ğŸ’¡ 1ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚ãŸã‚Šã®ã‚³ã‚¹ãƒˆï¼ˆéå»7æ—¥é–“ï¼‰"
echo ""
echo "  æ—¥ä»˜       | main+dev | kag      | å…¨ä½“"
echo "  -----------|----------|----------|----------"

# æ—¥åˆ¥ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°ã‚’mapã«æ ¼ç´
declare -A DAILY_SESSIONS_SANDBOX
declare -A DAILY_SESSIONS_KAG_MAP
declare -A DAILY_COST_SANDBOX
declare -A DAILY_COST_KAG_MAP

# main+devã®ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°ã‚’sandboxã¨ã—ã¦é›†è¨ˆ
while IFS= read -r line; do
  if [ -n "$line" ]; then
    DATE=$(echo "$line" | cut -d'|' -f1)
    SESSIONS=$(echo "$line" | cut -d'|' -f2)
    DAILY_SESSIONS_SANDBOX[$DATE]=$((${DAILY_SESSIONS_SANDBOX[$DATE]:-0} + SESSIONS))
  fi
done < <(jq -r '.results[] |
  (.[] | select(.field == "day_utc") | .value | split(" ")[0]) as $date |
  (.[] | select(.field == "sessions") | .value) as $sessions |
  "\($date)|\($sessions)"
' "$OUTPUT_DIR/daily_main.json" 2>/dev/null)

# devã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚‚åŠ ç®—
while IFS= read -r line; do
  if [ -n "$line" ]; then
    DATE=$(echo "$line" | cut -d'|' -f1)
    SESSIONS=$(echo "$line" | cut -d'|' -f2)
    DAILY_SESSIONS_SANDBOX[$DATE]=$((${DAILY_SESSIONS_SANDBOX[$DATE]:-0} + SESSIONS))
  fi
done < <(jq -r '.results[] |
  (.[] | select(.field == "day_utc") | .value | split(" ")[0]) as $date |
  (.[] | select(.field == "sessions") | .value) as $sessions |
  "\($date)|\($sessions)"
' "$OUTPUT_DIR/daily_dev.json" 2>/dev/null)

# kagã®ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°
while IFS= read -r line; do
  if [ -n "$line" ]; then
    DATE=$(echo "$line" | cut -d'|' -f1)
    SESSIONS=$(echo "$line" | cut -d'|' -f2)
    DAILY_SESSIONS_KAG_MAP[$DATE]=$SESSIONS
  fi
done < <(jq -r '.results[] |
  (.[] | select(.field == "day_utc") | .value | split(" ")[0]) as $date |
  (.[] | select(.field == "sessions") | .value) as $sessions |
  "\($date)|\($sessions)"
' "$OUTPUT_DIR/daily_kag.json" 2>/dev/null)

# sandboxã®æ—¥åˆ¥ã‚³ã‚¹ãƒˆ
while IFS= read -r line; do
  if [ -n "$line" ]; then
    DATE=$(echo "$line" | cut -d'|' -f1)
    COST=$(echo "$line" | cut -d'|' -f2)
    DAILY_COST_SANDBOX[$DATE]=$COST
  fi
done < <(jq -r '
  .ResultsByTime[] |
  .TimePeriod.Start as $date |
  ([.Groups[] | select(.Keys[0] | contains("Claude") or contains("Bedrock")) | .Metrics.UnblendedCost.Amount | tonumber] | add // 0) as $cost |
  "\($date)|\($cost)"
' "$OUTPUT_DIR/cost.json" 2>/dev/null)

# kagã®æ—¥åˆ¥ã‚³ã‚¹ãƒˆ
while IFS= read -r line; do
  if [ -n "$line" ]; then
    DATE=$(echo "$line" | cut -d'|' -f1)
    COST=$(echo "$line" | cut -d'|' -f2)
    DAILY_COST_KAG_MAP[$DATE]=$COST
  fi
done < <(jq -r '
  .ResultsByTime[] |
  .TimePeriod.Start as $date |
  ([.Groups[] | select(.Keys[0] | contains("Claude") or contains("Bedrock")) | .Metrics.UnblendedCost.Amount | tonumber] | add // 0) as $cost |
  "\($date)|\($cost)"
' "$OUTPUT_DIR/cost_kag.json" 2>/dev/null)

# æ—¥åˆ¥ã‚»ãƒƒã‚·ãƒ§ãƒ³å˜ä¾¡è¡¨ç¤º
CPS_SUM_COST_SB=0
CPS_SUM_COST_KG=0
CPS_SUM_SESS_SB=0
CPS_SUM_SESS_KG=0

for DATE in $(jq -r '.ResultsByTime[].TimePeriod.Start' "$OUTPUT_DIR/cost.json" | sort); do
  S_SB=${DAILY_SESSIONS_SANDBOX[$DATE]:-0}
  S_KG=${DAILY_SESSIONS_KAG_MAP[$DATE]:-0}
  C_SB=${DAILY_COST_SANDBOX[$DATE]:-0}
  C_KG=${DAILY_COST_KAG_MAP[$DATE]:-0}
  S_ALL=$((S_SB + S_KG))
  C_ALL=$(echo "$C_SB + $C_KG" | bc)

  if [ "$S_SB" -gt 0 ]; then
    CPS_SB=$(printf "%.2f" $(echo "scale=4; $C_SB / $S_SB" | bc))
  else
    CPS_SB="-"
  fi
  if [ "$S_KG" -gt 0 ]; then
    CPS_KG=$(printf "%.2f" $(echo "scale=4; $C_KG / $S_KG" | bc))
  else
    CPS_KG="-"
  fi
  if [ "$S_ALL" -gt 0 ]; then
    CPS_ALL=$(printf "%.2f" $(echo "scale=4; $C_ALL / $S_ALL" | bc))
  else
    CPS_ALL="-"
  fi

  printf "  %s | \$%-6s | \$%-6s | \$%-6s\n" "$DATE" "$CPS_SB" "$CPS_KG" "$CPS_ALL"

  CPS_SUM_COST_SB=$(echo "$CPS_SUM_COST_SB + $C_SB" | bc)
  CPS_SUM_COST_KG=$(echo "$CPS_SUM_COST_KG + $C_KG" | bc)
  CPS_SUM_SESS_SB=$((CPS_SUM_SESS_SB + S_SB))
  CPS_SUM_SESS_KG=$((CPS_SUM_SESS_KG + S_KG))
done

echo "  -----------|----------|----------|----------"

CPS_SUM_SESS_ALL=$((CPS_SUM_SESS_SB + CPS_SUM_SESS_KG))
CPS_SUM_COST_ALL=$(echo "$CPS_SUM_COST_SB + $CPS_SUM_COST_KG" | bc)
if [ "$CPS_SUM_SESS_SB" -gt 0 ]; then
  AVG_SB=$(printf "%.2f" $(echo "scale=4; $CPS_SUM_COST_SB / $CPS_SUM_SESS_SB" | bc))
else
  AVG_SB="-"
fi
if [ "$CPS_SUM_SESS_KG" -gt 0 ]; then
  AVG_KG=$(printf "%.2f" $(echo "scale=4; $CPS_SUM_COST_KG / $CPS_SUM_SESS_KG" | bc))
else
  AVG_KG="-"
fi
if [ "$CPS_SUM_SESS_ALL" -gt 0 ]; then
  AVG_ALL=$(printf "%.2f" $(echo "scale=4; $CPS_SUM_COST_ALL / $CPS_SUM_SESS_ALL" | bc))
else
  AVG_ALL="-"
fi
printf "  å¹³å‡       | \$%-6s | \$%-6s | \$%-6s\n" "$AVG_SB" "$AVG_KG" "$AVG_ALL"
echo ""
echo "  â€» æ–½ç­–å‰å‚è€ƒå€¤: \$0.58/å›"
echo ""

# ========================================
# Claudeãƒ¢ãƒ‡ãƒ« ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹æœï¼ˆä¸¡ã‚¢ã‚«ã‚¦ãƒ³ãƒˆåˆç®—ï¼‰
# ========================================

# --- Sonnet 4.5 ---
echo "ğŸ“Š Claude Sonnet 4.5 ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹æœ"

S_INPUT_COST=$(echo \
  "$(jq -r '[.ResultsByTime[].Groups[] | select(.Keys[0] | test("InputToken") and (test("Cache") | not)) | .Metrics.UnblendedCost.Amount | tonumber] | add // 0' "$OUTPUT_DIR/sonnet_usage.json")" \
  "+ $(jq -r '[.ResultsByTime[].Groups[] | select(.Keys[0] | test("InputToken") and (test("Cache") | not)) | .Metrics.UnblendedCost.Amount | tonumber] | add // 0' "$OUTPUT_DIR/sonnet_usage_kag.json")" \
  | bc)
S_OUTPUT_COST=$(echo \
  "$(jq -r '[.ResultsByTime[].Groups[] | select(.Keys[0] | test("OutputToken")) | .Metrics.UnblendedCost.Amount | tonumber] | add // 0' "$OUTPUT_DIR/sonnet_usage.json")" \
  "+ $(jq -r '[.ResultsByTime[].Groups[] | select(.Keys[0] | test("OutputToken")) | .Metrics.UnblendedCost.Amount | tonumber] | add // 0' "$OUTPUT_DIR/sonnet_usage_kag.json")" \
  | bc)
S_CACHE_READ_COST=$(echo \
  "$(jq -r '[.ResultsByTime[].Groups[] | select(.Keys[0] | test("CacheReadInputToken")) | .Metrics.UnblendedCost.Amount | tonumber] | add // 0' "$OUTPUT_DIR/sonnet_usage.json")" \
  "+ $(jq -r '[.ResultsByTime[].Groups[] | select(.Keys[0] | test("CacheReadInputToken")) | .Metrics.UnblendedCost.Amount | tonumber] | add // 0' "$OUTPUT_DIR/sonnet_usage_kag.json")" \
  | bc)
S_CACHE_WRITE_COST=$(echo \
  "$(jq -r '[.ResultsByTime[].Groups[] | select(.Keys[0] | test("CacheWriteInputToken")) | .Metrics.UnblendedCost.Amount | tonumber] | add // 0' "$OUTPUT_DIR/sonnet_usage.json")" \
  "+ $(jq -r '[.ResultsByTime[].Groups[] | select(.Keys[0] | test("CacheWriteInputToken")) | .Metrics.UnblendedCost.Amount | tonumber] | add // 0' "$OUTPUT_DIR/sonnet_usage_kag.json")" \
  | bc)

printf "  é€šå¸¸Input:   \$%.2f\n" $S_INPUT_COST
printf "  Output:      \$%.2f\n" $S_OUTPUT_COST
printf "  CacheRead:   \$%.2f\n" $S_CACHE_READ_COST
printf "  CacheWrite:  \$%.2f\n" $S_CACHE_WRITE_COST

# Sonnet ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡è¨ˆç®—ï¼ˆInput: $3/1M, CacheRead: $0.30/1Mï¼‰
if (( $(echo "$S_INPUT_COST > 0 || $S_CACHE_READ_COST > 0" | bc -l) )); then
  S_INPUT_TOKENS=$(echo "scale=0; $S_INPUT_COST / 0.000003" | bc)
  S_CACHE_READ_TOKENS=$(echo "scale=0; $S_CACHE_READ_COST / 0.0000003" | bc)
  S_TOTAL_INPUT_TOKENS=$(echo "$S_INPUT_TOKENS + $S_CACHE_READ_TOKENS" | bc)
  if [ "$S_TOTAL_INPUT_TOKENS" != "0" ]; then
    S_CACHE_HIT_RATE=$(echo "scale=1; $S_CACHE_READ_TOKENS * 100 / $S_TOTAL_INPUT_TOKENS" | bc)
    echo "  ğŸ“ˆ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡: ${S_CACHE_HIT_RATE}%"
    S_WOULD_HAVE_COST=$(echo "scale=2; $S_CACHE_READ_TOKENS * 0.000003" | bc)
    S_SAVINGS=$(echo "scale=2; $S_WOULD_HAVE_COST - $S_CACHE_READ_COST" | bc)
    S_NET_SAVINGS=$(echo "scale=2; $S_SAVINGS - $S_CACHE_WRITE_COST" | bc)
    printf "  ğŸ’° ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¯€ç´„é¡: \$%.2fï¼ˆCacheWriteè€ƒæ…®å¾Œ: \$%.2fï¼‰\n" $S_SAVINGS $S_NET_SAVINGS
  fi
fi
echo ""

# --- Opus 4.6 ---
O_INPUT_COST2=$(echo \
  "$(jq -r '[.ResultsByTime[].Groups[] | select(.Keys[0] | test("InputToken") and (test("Cache") | not)) | .Metrics.UnblendedCost.Amount | tonumber] | add // 0' "$OUTPUT_DIR/opus_usage.json")" \
  "+ $(jq -r '[.ResultsByTime[].Groups[] | select(.Keys[0] | test("InputToken") and (test("Cache") | not)) | .Metrics.UnblendedCost.Amount | tonumber] | add // 0' "$OUTPUT_DIR/opus_usage_kag.json")" \
  | bc)
O_OUTPUT_COST2=$(echo \
  "$(jq -r '[.ResultsByTime[].Groups[] | select(.Keys[0] | test("OutputToken")) | .Metrics.UnblendedCost.Amount | tonumber] | add // 0' "$OUTPUT_DIR/opus_usage.json")" \
  "+ $(jq -r '[.ResultsByTime[].Groups[] | select(.Keys[0] | test("OutputToken")) | .Metrics.UnblendedCost.Amount | tonumber] | add // 0' "$OUTPUT_DIR/opus_usage_kag.json")" \
  | bc)
O_CACHE_READ_COST2=$(echo \
  "$(jq -r '[.ResultsByTime[].Groups[] | select(.Keys[0] | test("CacheReadInputToken")) | .Metrics.UnblendedCost.Amount | tonumber] | add // 0' "$OUTPUT_DIR/opus_usage.json")" \
  "+ $(jq -r '[.ResultsByTime[].Groups[] | select(.Keys[0] | test("CacheReadInputToken")) | .Metrics.UnblendedCost.Amount | tonumber] | add // 0' "$OUTPUT_DIR/opus_usage_kag.json")" \
  | bc)
O_CACHE_WRITE_COST2=$(echo \
  "$(jq -r '[.ResultsByTime[].Groups[] | select(.Keys[0] | test("CacheWriteInputToken")) | .Metrics.UnblendedCost.Amount | tonumber] | add // 0' "$OUTPUT_DIR/opus_usage.json")" \
  "+ $(jq -r '[.ResultsByTime[].Groups[] | select(.Keys[0] | test("CacheWriteInputToken")) | .Metrics.UnblendedCost.Amount | tonumber] | add // 0' "$OUTPUT_DIR/opus_usage_kag.json")" \
  | bc)
O_TOTAL2=$(echo "$O_INPUT_COST2 + $O_OUTPUT_COST2 + $O_CACHE_READ_COST2 + $O_CACHE_WRITE_COST2" | bc)

if (( $(echo "$O_TOTAL2 > 0" | bc -l) )); then
  echo "ğŸ“Š Claude Opus 4.6 ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹æœ"
  printf "  é€šå¸¸Input:   \$%.2f\n" $O_INPUT_COST2
  printf "  Output:      \$%.2f\n" $O_OUTPUT_COST2
  printf "  CacheRead:   \$%.2f\n" $O_CACHE_READ_COST2
  printf "  CacheWrite:  \$%.2f\n" $O_CACHE_WRITE_COST2

  # Opus ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡è¨ˆç®—ï¼ˆInput: $15/1M, CacheRead: $1.50/1Mï¼‰
  if (( $(echo "$O_INPUT_COST2 > 0 || $O_CACHE_READ_COST2 > 0" | bc -l) )); then
    O_INPUT_TOKENS=$(echo "scale=0; $O_INPUT_COST2 / 0.000015" | bc)
    O_CACHE_READ_TOKENS=$(echo "scale=0; $O_CACHE_READ_COST2 / 0.0000015" | bc)
    O_TOTAL_INPUT_TOKENS=$(echo "$O_INPUT_TOKENS + $O_CACHE_READ_TOKENS" | bc)
    if [ "$O_TOTAL_INPUT_TOKENS" != "0" ]; then
      O_CACHE_HIT_RATE=$(echo "scale=1; $O_CACHE_READ_TOKENS * 100 / $O_TOTAL_INPUT_TOKENS" | bc)
      echo "  ğŸ“ˆ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡: ${O_CACHE_HIT_RATE}%"
      O_WOULD_HAVE_COST=$(echo "scale=2; $O_CACHE_READ_TOKENS * 0.000015" | bc)
      O_SAVINGS=$(echo "scale=2; $O_WOULD_HAVE_COST - $O_CACHE_READ_COST2" | bc)
      O_NET_SAVINGS=$(echo "scale=2; $O_SAVINGS - $O_CACHE_WRITE_COST2" | bc)
      printf "  ğŸ’° ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¯€ç´„é¡: \$%.2fï¼ˆCacheWriteè€ƒæ…®å¾Œ: \$%.2fï¼‰\n" $O_SAVINGS $O_NET_SAVINGS
    fi
  fi
  echo ""
fi

# ========================================
# é€±æ¬¡ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆv0.1ãƒªãƒªãƒ¼ã‚¹ä»¥é™ï¼‰
# ========================================
echo "ğŸ“… é€±æ¬¡ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆãƒªãƒªãƒ¼ã‚¹ä»¥é™ï¼‰"
echo ""

# jqã§æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿ã«é€±ç•ªå·ã‚’ä»˜ã‘ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
jq -r '.results[] |
  (.[] | select(.field == "day_utc") | .value | split(" ")[0]) as $date |
  (.[] | select(.field == "sessions") | .value) as $sessions |
  "\($date)|\($sessions)"
' "$OUTPUT_DIR/weekly_main.json" 2>/dev/null | while read line; do
  DATE=$(echo "$line" | cut -d'|' -f1)
  SESSIONS=$(echo "$line" | cut -d'|' -f2)
  WEEK=$(date -j -f "%Y-%m-%d" "$DATE" "+%Y-W%W" 2>/dev/null)
  echo "$WEEK|main|$SESSIONS"
done > "$OUTPUT_DIR/weekly_sessions.tmp"

jq -r '.results[] |
  (.[] | select(.field == "day_utc") | .value | split(" ")[0]) as $date |
  (.[] | select(.field == "sessions") | .value) as $sessions |
  "\($date)|\($sessions)"
' "$OUTPUT_DIR/weekly_kag.json" 2>/dev/null | while read line; do
  DATE=$(echo "$line" | cut -d'|' -f1)
  SESSIONS=$(echo "$line" | cut -d'|' -f2)
  WEEK=$(date -j -f "%Y-%m-%d" "$DATE" "+%Y-W%W" 2>/dev/null)
  echo "$WEEK|kag|$SESSIONS"
done >> "$OUTPUT_DIR/weekly_sessions.tmp"

# sandbox ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ã‚³ã‚¹ãƒˆ
jq -r '
  .ResultsByTime[] |
  .TimePeriod.Start as $date |
  ([.Groups[] | select(.Keys[0] | contains("Claude") or contains("Bedrock")) | .Metrics.UnblendedCost.Amount | tonumber] | add // 0) as $cost |
  "\($date)|\($cost)"
' "$OUTPUT_DIR/weekly_cost.json" 2>/dev/null | while read line; do
  DATE=$(echo "$line" | cut -d'|' -f1)
  COST=$(echo "$line" | cut -d'|' -f2)
  WEEK=$(date -j -f "%Y-%m-%d" "$DATE" "+%Y-W%W" 2>/dev/null)
  echo "$WEEK|cost|$COST"
done >> "$OUTPUT_DIR/weekly_sessions.tmp"

# kag-sandbox ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ã‚³ã‚¹ãƒˆ
jq -r '
  .ResultsByTime[] |
  .TimePeriod.Start as $date |
  ([.Groups[] | select(.Keys[0] | contains("Claude") or contains("Bedrock")) | .Metrics.UnblendedCost.Amount | tonumber] | add // 0) as $cost |
  "\($date)|\($cost)"
' "$OUTPUT_DIR/weekly_cost_kag.json" 2>/dev/null | while read line; do
  DATE=$(echo "$line" | cut -d'|' -f1)
  COST=$(echo "$line" | cut -d'|' -f2)
  WEEK=$(date -j -f "%Y-%m-%d" "$DATE" "+%Y-W%W" 2>/dev/null)
  echo "$WEEK|cost|$COST"
done >> "$OUTPUT_DIR/weekly_sessions.tmp"

echo "  é€±        | main | kag  | åˆè¨ˆ |  ã‚³ã‚¹ãƒˆ"
echo "  ----------|------|------|------|--------"

# é€±ã”ã¨ã«é›†è¨ˆã—ã¦è¡¨ç¤º
cat "$OUTPUT_DIR/weekly_sessions.tmp" | cut -d'|' -f1 | sort -u | while read WEEK; do
  if [ -n "$WEEK" ]; then
    W_MAIN=$(grep "^$WEEK|main|" "$OUTPUT_DIR/weekly_sessions.tmp" | cut -d'|' -f3 | tr '\n' '+' | sed 's/+$/\n/' | bc 2>/dev/null || echo 0)
    W_MAIN=${W_MAIN:-0}
    W_KAG=$(grep "^$WEEK|kag|" "$OUTPUT_DIR/weekly_sessions.tmp" | cut -d'|' -f3 | tr '\n' '+' | sed 's/+$/\n/' | bc 2>/dev/null || echo 0)
    W_KAG=${W_KAG:-0}
    W_COST=$(grep "^$WEEK|cost|" "$OUTPUT_DIR/weekly_sessions.tmp" | cut -d'|' -f3 | tr '\n' '+' | sed 's/+$/\n/' | bc 2>/dev/null || echo 0)
    W_COST=$(printf "%.0f" ${W_COST:-0})
    W_TOTAL=$((W_MAIN + W_KAG))
    printf "  %-9s | %4d | %4d | %4d | \$%s\n" "$WEEK" "$W_MAIN" "$W_KAG" "$W_TOTAL" "$W_COST"
  fi
done

rm -f "$OUTPUT_DIR/weekly_sessions.tmp"
echo ""

# ========================================
# Tavily API åˆ©ç”¨çŠ¶æ³
# ========================================
if [ "$TAVILY_KEY_COUNT" -gt 0 ]; then
  echo "ğŸ” Tavily API åˆ©ç”¨çŠ¶æ³"
  echo ""
  echo "  ã‚­ãƒ¼  | ä½¿ç”¨é‡ | ä¸Šé™   | æ®‹ã‚Š   | çŠ¶æ…‹"
  echo "  ------|--------|--------|--------|------"

  TAVILY_TOTAL_USED=0
  TAVILY_TOTAL_LIMIT=0

  for i in $(seq 1 $TAVILY_KEY_COUNT); do
    FILE="$OUTPUT_DIR/tavily_key${i}.json"
    if [ -f "$FILE" ] && [ -s "$FILE" ]; then
      USED=$(jq -r '.key.usage // 0' "$FILE" 2>/dev/null)
      LIMIT=$(jq -r '.account.plan_limit // 0' "$FILE" 2>/dev/null)
      [ "$USED" = "null" ] && USED=0
      [ "$LIMIT" = "null" ] && LIMIT=1000
      REMAINING=$((LIMIT - USED))
      [ $REMAINING -lt 0 ] && REMAINING=0

      if [ $REMAINING -le 0 ]; then
        STATUS="æ¯æ¸‡"
      elif [ $REMAINING -le 100 ]; then
        STATUS="æ®‹å°‘"
      else
        STATUS="OK"
      fi

      printf "  KEY%-2d | %6d | %6d | %6d | %s\n" "$i" "$USED" "$LIMIT" "$REMAINING" "$STATUS"

      TAVILY_TOTAL_USED=$((TAVILY_TOTAL_USED + USED))
      TAVILY_TOTAL_LIMIT=$((TAVILY_TOTAL_LIMIT + LIMIT))
    fi
  done

  TAVILY_TOTAL_REMAINING=$((TAVILY_TOTAL_LIMIT - TAVILY_TOTAL_USED))
  [ $TAVILY_TOTAL_REMAINING -lt 0 ] && TAVILY_TOTAL_REMAINING=0
  echo "  ------|--------|--------|--------|------"
  printf "  åˆè¨ˆ  | %6d | %6d | %6d |\n" "$TAVILY_TOTAL_USED" "$TAVILY_TOTAL_LIMIT" "$TAVILY_TOTAL_REMAINING"

  # æ—¥å¹³å‡æ¶ˆè²»ã®æ¨å®šï¼ˆå…¨ä½“ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°ã‹ã‚‰é€†ç®—: ã‚»ãƒƒã‚·ãƒ§ãƒ³â‰’æ¤œç´¢å›æ•°ï¼‰
  TOTAL_SESSIONS_ALL=$((TOTAL_MAIN + TOTAL_KAG + TOTAL_DEV))
  if [ "$TOTAL_SESSIONS_ALL" -gt 0 ]; then
    DAYS_WITH_DATA=$(jq '.ResultsByTime | length' "$OUTPUT_DIR/cost.json")
    [ "$DAYS_WITH_DATA" -lt 1 ] && DAYS_WITH_DATA=1
    DAILY_CREDITS=$(echo "scale=0; $TOTAL_SESSIONS_ALL / $DAYS_WITH_DATA" | bc)
    [ "$DAILY_CREDITS" -lt 1 ] && DAILY_CREDITS=1
  else
    DAILY_CREDITS=53  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å€¤ï¼ˆæœ€é©åŒ–å¾Œå®Ÿæ¸¬å€¤ï¼‰
  fi

  if [ $TAVILY_TOTAL_REMAINING -gt 0 ] && [ "$DAILY_CREDITS" -gt 0 ]; then
    DAYS_LEFT=$((TAVILY_TOTAL_REMAINING / DAILY_CREDITS))
    EXHAUST_DATE=$(date -v+${DAYS_LEFT}d +%Y-%m-%d)
    echo ""
    echo "  æ—¥å¹³å‡æ¶ˆè²»: ${DAILY_CREDITS}ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆ/æ—¥ï¼ˆç›´è¿‘7æ—¥é–“ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°ãƒ™ãƒ¼ã‚¹ï¼‰"
    echo "  æ¯æ¸‡äºˆæ¸¬: ç´„${DAYS_LEFT}æ—¥å¾Œï¼ˆ${EXHAUST_DATE}é ƒï¼‰"
  elif [ $TAVILY_TOTAL_REMAINING -le 0 ]; then
    echo ""
    echo "  âš ï¸  å…¨ã‚­ãƒ¼ãŒæ¯æ¸‡ã—ã¦ã„ã¾ã™"
  fi
  echo ""
fi

echo "âœ… å®Œäº†ï¼"
```

## å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ

ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œå¾Œã€ä»¥ä¸‹ã®æƒ…å ±ãŒå‡ºåŠ›ã•ã‚Œã‚‹ï¼š

1. **ç›´è¿‘12æ™‚é–“ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°**: æ™‚é–“å¸¯åˆ¥ã®è¡¨å½¢å¼ï¼ˆmain/kag/devï¼‰
2. **Cognitoãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°**: ç’°å¢ƒã”ã¨ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°ï¼ˆmain/kagï¼‰+ kagæœ€è¿‘è¿½åŠ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®Emailä¸€è¦§
3. **æ—¥æ¬¡ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°**: éå»7æ—¥é–“ã®æ—¥åˆ¥å›æ•°ï¼ˆmain/kag/devåˆ¥ï¼‰
4. **æ™‚é–“åˆ¥ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°**: ç›´è¿‘24æ™‚é–“ã®å…¨æ™‚é–“å¸¯ï¼ˆASCIIãƒãƒ¼ã‚°ãƒ©ãƒ•ãƒ»JSTè¡¨ç¤ºã€main/kag/devï¼‰
5. **Bedrockã‚³ã‚¹ãƒˆï¼ˆæ—¥åˆ¥ï¼‰**: éå»7æ—¥é–“ã®æ—¥åˆ¥ã‚³ã‚¹ãƒˆï¼ˆmain+dev/kagåˆ¥ï¼‰
6. **Bedrockã‚³ã‚¹ãƒˆå†…è¨³ï¼ˆç’°å¢ƒåˆ¥ x ãƒ¢ãƒ‡ãƒ«åˆ¥ï¼‰**: ã‚¢ã‚«ã‚¦ãƒ³ãƒˆåˆ¥å®Ÿã‚³ã‚¹ãƒˆã«ã‚ˆã‚‹ç’°å¢ƒåˆ¥ãƒ»ãƒ¢ãƒ‡ãƒ«åˆ¥ã‚³ã‚¹ãƒˆè¡¨ï¼ˆé€±é–“ãƒ»æœˆé–“æ¨å®šä»˜ãï¼‰
7. **1ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚ãŸã‚Šã®ã‚³ã‚¹ãƒˆ**: æ—¥åˆ¥ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³å˜ä¾¡ï¼ˆmain+dev/kag/å…¨ä½“ï¼‰ã¨å¹³å‡å€¤ã€‚ã‚³ã‚¹ãƒˆå‰Šæ¸›æ–½ç­–ã®åŠ¹æœç¢ºèªç”¨
8. **Claudeãƒ¢ãƒ‡ãƒ« ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹æœ**: Sonnet 4.5 / Opus 4.6 å„ãƒ¢ãƒ‡ãƒ«ã®Input/Output/CacheRead/CacheWriteã®å†…è¨³ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ã€ç¯€ç´„é¡ï¼ˆä¸¡ã‚¢ã‚«ã‚¦ãƒ³ãƒˆåˆç®—ï¼‰
9. **é€±æ¬¡ãƒˆãƒ¬ãƒ³ãƒ‰**: ãƒªãƒªãƒ¼ã‚¹ä»¥é™ã®é€±ã”ã¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°ã¨ã‚³ã‚¹ãƒˆã®æ¨ç§»ï¼ˆéå»4é€±é–“ã€ä¸¡ã‚¢ã‚«ã‚¦ãƒ³ãƒˆåˆç®—ï¼‰
10. **Tavily API åˆ©ç”¨çŠ¶æ³**: ã‚­ãƒ¼åˆ¥ã®ä½¿ç”¨é‡/ä¸Šé™/æ®‹ã‚Šã€æ—¥å¹³å‡æ¶ˆè²»ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã€æ¯æ¸‡äºˆæ¸¬æ—¥

## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
sandbox ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ (715841358122)
â”œâ”€â”€ Cognito: marp-main ãƒ—ãƒ¼ãƒ«
â”œâ”€â”€ AgentCore: marp_agent_main, marp_agent_dev
â””â”€â”€ Bedrock: main + dev ã®ã‚³ã‚¹ãƒˆ

kag-sandbox ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ (105778051969)
â”œâ”€â”€ Cognito: amplifyAuthUserPoolï¼ˆCloudFormationå‡ºåŠ›ã§ç‰¹å®šï¼‰
â”œâ”€â”€ AgentCore: marp_agent_mainï¼ˆkagãƒªãƒã®mainãƒ–ãƒ©ãƒ³ãƒï¼‰
â””â”€â”€ Bedrock: kag ã®ã‚³ã‚¹ãƒˆ
```

## æŠ€è¡“è©³ç´°

### ãƒãƒ«ãƒã‚¢ã‚«ã‚¦ãƒ³ãƒˆå¯¾å¿œ

mainã¨kagã¯ç•°ãªã‚‹AWSã‚¢ã‚«ã‚¦ãƒ³ãƒˆã§é‹ç”¨ã•ã‚Œã¦ã„ã‚‹ã€‚ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ `PROFILE_MAIN` ã¨ `PROFILE_KAG` ã®2ã¤ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ã„åˆ†ã‘ã‚‹ã€‚

- **sandbox**: mainç’°å¢ƒ + devç’°å¢ƒã®ãƒªã‚½ãƒ¼ã‚¹ã¨ã‚³ã‚¹ãƒˆ
- **kag-sandbox**: kagç’°å¢ƒã®ãƒªã‚½ãƒ¼ã‚¹ã¨ã‚³ã‚¹ãƒˆ

SSOã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒåˆ‡ã‚Œã¦ã„ã‚‹å ´åˆã€ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒè‡ªå‹•çš„ã« `aws sso login` ã‚’å®Ÿè¡Œã™ã‚‹ã€‚kag-sandboxã®ãƒ­ã‚°ã‚¤ãƒ³ã«å¤±æ•—ã—ãŸå ´åˆã®ã¿ã€kagã®ãƒ‡ãƒ¼ã‚¿ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã‚‹ã€‚

### ã‚³ã‚¹ãƒˆè¨ˆç®—æ–¹æ³•

- **ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆé™¤å¤–**: Cost Explorer APIã§ `RECORD_TYPE=Usage` ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’é©ç”¨ã—ã€AWSã‚¯ãƒ¬ã‚¸ãƒƒãƒˆé©ç”¨å‰ã®å®Ÿåˆ©ç”¨ã‚³ã‚¹ãƒˆã‚’å–å¾—
- **kag**: kag-sandboxã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®å®Ÿã‚³ã‚¹ãƒˆï¼ˆæ¨å®šãªã—ï¼‰
- **main/dev**: sandboxã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ã‚³ã‚¹ãƒˆã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³æ¯”ç‡ã§æŒ‰åˆ†ï¼ˆdevã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒãªã„å ´åˆã¯å…¨é¡mainï¼‰

### OTELãƒ­ã‚°å½¢å¼ã¸ã®å¯¾å¿œ

AgentCoreã®ãƒ­ã‚°ã¯ `otel-rt-logs` ã‚¹ãƒˆãƒªãƒ¼ãƒ ã«OTELå½¢å¼ã§å‡ºåŠ›ã•ã‚Œã‚‹ã€‚å„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã¯ `session.id` ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã§è­˜åˆ¥ã•ã‚Œã‚‹ãŸã‚ã€`count_distinct(sid)` ã§ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆã™ã‚‹ã€‚

### ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³å¤‰æ›

CloudWatch Logs Insightsã§ `datefloor(@timestamp + 9h, ...)` ã‚’ä½¿ã†ã¨æŒ™å‹•ãŒä¸å®‰å®šãªãŸã‚ã€UTCã®ã¾ã¾é›†è¨ˆã—ã¦ã‹ã‚‰ã‚¹ã‚¯ãƒªãƒ—ãƒˆå´ã§JSTã«å¤‰æ›ã—ã¦ã„ã‚‹ã€‚

### kag-sandbox ã® Cognito ãƒ—ãƒ¼ãƒ«ç‰¹å®š

kag-sandboxã‚¢ã‚«ã‚¦ãƒ³ãƒˆã§ã¯Cognitoãƒ—ãƒ¼ãƒ«åãŒæ±ç”¨çš„ï¼ˆ`amplifyAuthUserPool*`ï¼‰ãªãŸã‚ã€`marp-kag` ã®ã‚ˆã†ãªåå‰æ¤œç´¢ãŒã§ããªã„ã€‚ä»£ã‚ã‚Šã«CloudFormationå‡ºåŠ›ã‹ã‚‰Amplifyã‚¢ãƒ—ãƒª `dt1uykzxnkuoh` ã«ç´ã¥ããƒ—ãƒ¼ãƒ«IDã‚’å–å¾—ã—ã¦ã„ã‚‹ã€‚

## æ³¨æ„äº‹é …

- SSOã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒåˆ‡ã‚Œã¦ã„ã‚‹å ´åˆã¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒè‡ªå‹•çš„ã« `aws sso login` ã‚’å®Ÿè¡Œã™ã‚‹ï¼ˆãƒ–ãƒ©ã‚¦ã‚¶èªè¨¼ãŒå¿…è¦ï¼‰
- kag-sandbox ã®ãƒ­ã‚°ã‚¤ãƒ³ã«å¤±æ•—ã—ãŸå ´åˆã®ã¿ã€kagã®ãƒ‡ãƒ¼ã‚¿ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã‚‹
- CloudWatch Logsã‚¯ã‚¨ãƒªã¯éåŒæœŸã®ãŸã‚10ç§’å¾…æ©Ÿã—ã¦ã„ã‚‹ï¼ˆå¿…è¦ã«å¿œã˜ã¦èª¿æ•´ï¼‰

## å›ç­”æ™‚ã®è¡¨ç¤ºãƒ«ãƒ¼ãƒ«

ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œå¾Œã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®å›ç­”ã§ã¯ä»¥ä¸‹ã‚’å®ˆã‚‹ã“ã¨ï¼š

1. **ç›´è¿‘12æ™‚é–“ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°**: ã‚µãƒãƒªãƒ¼ã›ãšã€ã‚¹ã‚¯ãƒªãƒ—ãƒˆå‡ºåŠ›ã®è¡¨å½¢å¼ã‚’ãã®ã¾ã¾Markdownãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã—ã¦è¡¨ç¤ºã™ã‚‹
2. **1ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚ãŸã‚Šã®ã‚³ã‚¹ãƒˆ**: ã‚µãƒãƒªãƒ¼ã›ãšã€æ—¥åˆ¥ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ãã®ã¾ã¾Markdownãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã—ã¦è¡¨ç¤ºã™ã‚‹
3. **Tavily API åˆ©ç”¨çŠ¶æ³**: ã‚µãƒãƒªãƒ¼ã›ãšã€ã‚­ãƒ¼åˆ¥ãƒ†ãƒ¼ãƒ–ãƒ«ã¨æ¯æ¸‡äºˆæ¸¬ã‚’ãã®ã¾ã¾è¡¨ç¤ºã™ã‚‹
4. ãã®ä»–ã®ãƒ‡ãƒ¼ã‚¿ã¯é©å®œã‚µãƒãƒªãƒ¼ã—ã¦OK
